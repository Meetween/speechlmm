# @package _global_

defaults:
  - _self_

wandb_project: speechlmm
deepspeed_config: ${oc.env:SPEECHLMM_ROOT}/conf/deepspeed/zero3.json

data:
  data_config_path: ${oc.env:SPEECHLMM_ROOT}/conf/datasets/delivarable/speech_in.yml
  rebuild_dataset_cache: false
  num_proc_for_preprocessing: 32

  task_weights:
    TSUM: 0.1
    SSUM: 0.1
    MT: 0.1
    TextInstruct: 0.2
    ASR: 0.2
    ST: 0.2
    SLU_INTENT_ONLY: 0.1

  multi_task_sampler: "alternating"

  group_dataset_by_task:
    train: true

training:
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 16
  save_steps: 500
  save_total_limit: 10
  num_train_epochs: 1
  eval_steps: 1000
  eval_strategy: "no"
  eval_num_batched_generations: 1
  attn_implementation: flash_attention_2
  dataloader_num_workers: 4

  learning_rate: 5.0e-5

  freeze_modules:
    - audio_encoder

  group_by_modality_length: false
  report_to: wandb
  pretrained_checkpoint: ${oc.env:CHECKPOINTS_HOME}/speechlmm/llava-pretrain-audio-seamless-mlp-llama_3_1-delivarable/speechlmm_in_warmup/checkpoint-4000

model:
  add_all_multimodal_tokens: true

  audio_adapter:
    hidden_layers: 4


  chunk_size_in_seconds: 15
  chunk_overlap_in_seconds: 1
  chunk_encoding_strategy: loop


# python speechlmm/train/train_hydra.py \
#     --config-name pretrain \
#     model/audio_encoder=seamless \
#     model/audio_adapter=mlp \
#     model/text_decoder=llama_3_8b \
#     training_setting=delivarable/speechlmm_in
